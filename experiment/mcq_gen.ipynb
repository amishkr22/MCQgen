{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amish\\AppData\\Local\\Temp\\ipykernel_16364\\2848416455.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(openai_api_key=key,model_name='gpt-4',temperature=0.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C37D78E130>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C37D79C2E0>, model_name='gpt-4', temperature=0.5, model_kwargs={}, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=key,model_name='gpt-4',temperature=0.5)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SequentialChain,LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = {\n",
    "    '1':{\n",
    "        'mcq':'multiple choice question',\n",
    "        'options':{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here'\n",
    "        },\n",
    "        'response':'correct answer'\n",
    "    },\n",
    "    '2':{\n",
    "        'mcq':'multiple choice question',\n",
    "        'options':{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here'\n",
    "        },\n",
    "        'response':'correct answer'\n",
    "    },\n",
    "    '3':{\n",
    "        'mcq':'multiple choice question',\n",
    "        'options':{\n",
    "            'a':'choice here',\n",
    "            'b':'choice here',\n",
    "            'c':'choice here',\n",
    "            'd':'choice here'\n",
    "        },\n",
    "        'response':'correct answer'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE='''\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given bthe above text , it is your job to\\\n",
    "create a quiz of {number} multiple choice question for {subject} students to {tone} tone.\n",
    "Make sure the question are not repeated and check all the question to be confirming the text as well.\n",
    "Make sure to format your response like RESPONSE_JSON and use it as a guide.\\\n",
    "Ensure to make {number} MCQs\n",
    "###RESPONSE_JSON\n",
    "{response_json}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_prompt = PromptTemplate(\n",
    "    input_variables=['text','number','subject','tone','response_json'],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amish\\AppData\\Local\\Temp\\ipykernel_16364\\1033466357.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  quiz_chain=LLMChain(llm=llm,prompt=quiz_prompt,output_key='quiz',verbose=True)\n"
     ]
    }
   ],
   "source": [
    "quiz_chain=LLMChain(llm=llm,prompt=quiz_prompt,output_key='quiz',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=['subject','quiz'],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm=llm,prompt=quiz_evaluation_prompt,output_key='review',verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain = SequentialChain(chains=[quiz_chain,review_chain],input_variables=['text','number','subject','tone','response_json'],\n",
    "                                          output_variables=['quiz','review'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\amish\\Downloads\\IEEE_Conference_Template.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader(file_path)\n",
    "page = reader.pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fairness and Bias in Large Language\n",
      "Models: A Comparative Analysis\n",
      "1stGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID2ndGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID3rdGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID\n",
      "4thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID5thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID6thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID\n",
      "Abstract —The rapid development and deployment of large lan-\n",
      "guage models (LLMs) have transformed various fields, including\n",
      "natural language processing, healthcare, and criminal justice.\n",
      "However, these models have raised significant concerns regarding\n",
      "fairness, equity, and bias, particularly in high-stakes applications\n",
      "where biased predictions can have adverse consequences. This\n",
      "study systematically evaluates the fairness and bias of LLMs\n",
      "developed by prominent organizations such as Meta, Mistral,\n",
      "NousResearch, and Qwen. Using benchmark datasets such as\n",
      "CivilComments and COMPAS, and employing advanced fairness\n",
      "metrics, we assess models’ predictive performance and biases\n",
      "across demographic groups. Metrics including Demographic\n",
      "Parity, True Positive Rate (TPR) Parity, False Positive Rate (FPR)\n",
      "Parity, Conditional Use Accuracy Equality (CUAE), and False\n",
      "Discovery Rate (FDR) Parity are used to quantify disparities in\n",
      "model predictions. The empirical results reveal varying degrees\n",
      "of bias among the LLMs, with Hermes models consistently\n",
      "demonstrating higher fairness compared to other evaluated\n",
      "models. This research provides critical insights into the fairness\n",
      "of LLMs, underscoring the need for responsible development\n",
      "and deployment of AI systems to mitigate potential biases and\n",
      "promote equitable outcomes.\n",
      "Index Terms —Fairness, Bias, Large Language Models, Civil-\n",
      "Comments, COMPAS, Demographic Parity, True Positive Rate\n",
      "Parity, False Positive Rate Parity, Machine Learning, Ethical AI,\n",
      "Aequitas.\n",
      "I. I NTRODUCTION\n",
      "The development and deployment of Machine Learning\n",
      "(ML) and Artificial Intelligence (AI) systems in high-stakes\n",
      "applications, such as criminal justice, healthcare, and hiring,\n",
      "have raised pressing concerns about fairness, equity, and\n",
      "bias. As these systems increasingly impact decisions affecting\n",
      "individuals and communities, mitigating biases related to race,\n",
      "gender, age, and other sensitive attributes has become essen-\n",
      "tial. The presence of unfair biases in these models can perpet-\n",
      "uate systemic inequalities, leading to adverse consequences\n",
      "such as racial discrimination, sexism, and ageism.\n",
      "Fairness in ML models is often assessed through metrics\n",
      "that evaluate the treatment and outcomes for different demo-graphic groups. For instance, demographic parity, also known\n",
      "as statistical parity, is a fairness criterion that requires the\n",
      "probability of a positive prediction to be equal across groups:\n",
      "P(ˆY= 1 |A=a) =P(ˆY= 1 |A=b)\n",
      "where A represents the sensitive attribute,such as race or\n",
      "gender.\n",
      "When this criterion is not met, it indicates potential bias\n",
      "in the model’s predictions for certain groups. In this study,\n",
      "we evaluate large language models (LLMs) from leading re-\n",
      "search organizations, including Meta, Mistral, NousResearch,\n",
      "and Qwen, using state-of-the-art fairness and bias assessment\n",
      "frameworks. By applying these frameworks, we aim to quan-\n",
      "tify the extent of fairness and bias in model predictions. We\n",
      "assess metrics such as True Positive Rate (TPR) Parity and\n",
      "False Positive Rate (FPR) Parity, which evaluate whether the\n",
      "rates of correct positive predictions and incorrect positive\n",
      "predictions are consistent across groups:\n",
      "TPR Parity:P(ˆY= 1 |Y= 1, A=a)\n",
      "P(ˆY= 1 |Y= 1, A=b)= 1 (1)\n",
      "FPR Parity:P(ˆY= 1 |Y= 0, A=a)\n",
      "P(ˆY= 1 |Y= 0, A=b)= 1 (2)\n",
      "where Y is the true label and ˆYis the predicted label.\n",
      "By examining Conditional Use Accuracy Equality (CUAE)\n",
      "and False Discovery Rate (FDR) Parity, we further explore\n",
      "how accurately the model’s positive predictions reflect the\n",
      "ground truth across demographic groups, highlighting potential\n",
      "disparities in the model’s decision boundaries:\n",
      "CUAE:P(Y= 1 |ˆY= 1, A=a)\n",
      "P(Y= 1 |ˆY= 1, A=b)= 1 (3)\n",
      "FDR Parity:P(Y= 0 |ˆY= 1, A=a)\n",
      "P(Y= 0 |ˆY= 1, A=b)= 1 (4)\n",
      "Our research leverages advanced fairness frameworks to\n",
      "systematically evaluate these metrics across datasets used in\n",
      "training and fine-tuning each model. By assessing LLMs from\n",
      "Meta, Mistral, NousResearch, and Qwen, we provide insights\n",
      "into the fairness of these models, the sources of potential\n",
      "bias in their training datasets, and the broader implications\n",
      "for equitable AI.\n",
      "II. B ACKGROUND AND RELATED WORK\n",
      "The rapid advancement of large language models (LLMs)\n",
      "has revolutionized natural language processing, providing high\n",
      "accuracy across various tasks such as text generation, sen-\n",
      "timent analysis, and classification. These models, including\n",
      "OpenAI’s GPT series, Meta’s LLaMa, and others, are exten-\n",
      "sively used across industries, but their deployment has raised\n",
      "significant ethical concerns. Specifically, LLMs are prone to\n",
      "inherit and amplify biases present in the large-scale datasets\n",
      "on which they are trained. Bias in LLMs reflects societal\n",
      "and historical inequalities, manifesting across multiple demo-\n",
      "graphic attributes such as gender, race, age, and socioeconomic\n",
      "status. Addressing these biases is crucial, as biased models\n",
      "can lead to unfair or harmful outcomes, especially in sensitive\n",
      "applications(e.g., healthcare, hiring, and law enforcement).\n",
      "1) Bias and Fairness Taxonomies in LLMs: Several studies\n",
      "have attempted to formalize and categorize bias and fairness\n",
      "in LLMs. Gallegos et al. (2023) consolidated definitions of\n",
      "social bias and fairness specifically for NLP tasks, proposing\n",
      "three taxonomies for bias evaluation: metrics, datasets, and\n",
      "mitigation techniques. The authors argue that fairness should\n",
      "be defined not only through quantitative metrics but also by\n",
      "considering the broader social impacts of model outputs. Their\n",
      "taxonomy organizes metrics by level, such as embedding-\n",
      "based, probability-based, and text-based metrics, allowing re-\n",
      "searchers to choose appropriate evaluation techniques based on\n",
      "access to model outputs. Similarly, Bouchard (2024) proposed\n",
      "an actionable framework for aligning LLM use cases with\n",
      "suitable bias and fairness metrics, focusing on context-specific\n",
      "risk assessment for LLM applications.\n",
      "2) Existing Frameworks for Bias Evaluation: Current meth-\n",
      "ods for evaluating bias in LLMs range from simple bench-\n",
      "marks to sophisticated frameworks. Zhao et al. introduced\n",
      "GPTBIAS , a framework that leverages advanced LLMs (such\n",
      "as GPT-4) for detecting specific bias types, including gen-\n",
      "der, race, and age biases. GPTBIAS uses specially designed\n",
      "prompts to expose biases in model responses, providing de-\n",
      "tailed information on bias types, affected demographics, and\n",
      "suggestions for mitigating bias. Additionally, Oketunji et al.\n",
      "developed the LLM Bias Index (LLMBI) , a metric designedto quantify bias in LLMs systematically across multiple di-\n",
      "mensions. LLMBI incorporates both demographic information\n",
      "and sentiment analysis to capture subtle biases, offering a\n",
      "composite score that reflects the extent of bias in various\n",
      "categories.\n",
      "3) Limitations of Existing Bias Metrics and Datasets:\n",
      "Many traditional bias evaluation techniques, such as word em-\n",
      "beddings or probability-based metrics, have limited robustness\n",
      "and interpretability when applied to complex scenarios, such\n",
      "as long-text generation. Jeung et al. introduced the Long Text\n",
      "Fairness Test (LTF-TEST) , which focuses on identifying\n",
      "biases that arise in extended text outputs. LTF-TEST evaluates\n",
      "model responses to paired prompts, allowing for a more\n",
      "nuanced assessment of bias in scenarios where simpler metrics\n",
      "may fall short. This approach highlights the importance of\n",
      "considering bias in long-form content, as biases can manifest\n",
      "differently compared to shorter or simpler tasks.\n",
      "4) Novel Approaches to Bias Mitigation: While numerous\n",
      "methods have been proposed to mitigate bias, their effective-\n",
      "ness varies based on the model and application. Techniques\n",
      "such as instruction fine-tuning and prompt engineering\n",
      "have shown promise in reducing biases by guiding the model\n",
      "towards more neutral responses. For example, REGARD-FT,\n",
      "a fine-tuning approach, pairs biased prompts with neutral\n",
      "responses, effectively reducing gender bias and improving\n",
      "model performance on fairness benchmarks. However, these\n",
      "techniques may still be limited in mitigating all forms of bias,\n",
      "especially when applied to proprietary or black-box models.\n",
      "5) Summary and Contributions: The existing literature re-\n",
      "veals substantial progress in bias evaluation and mitigation\n",
      "for LLMs, yet there remain significant challenges, particu-\n",
      "larly regarding interpretability, dataset alignment, and domain-\n",
      "specific application. Our research builds on these foundations\n",
      "by applying state-of-the-art bias and fairness frameworks to\n",
      "evaluate multiple open-source LLMs from various organiza-\n",
      "tions, including Meta, Mistral, NousResearch, and Qwen. Our\n",
      "study assesses these models across diverse datasets, examining\n",
      "how their training data and architecture influence bias. This\n",
      "work contributes to the ongoing efforts to develop fairer\n",
      "and more equitable AI systems, highlighting the nuances and\n",
      "limitations of current bias evaluation frameworks.\n",
      "III. M ETHODOLOGY\n",
      "In this study, we adopted a rigorous methodology to evaluate\n",
      "fairness and bias in large language models (LLMs) developed\n",
      "by multiple leading research organizations. Our evaluation\n",
      "framework consisted of two core components: dataset selection\n",
      "and model assessment using advanced fairness metrics. This\n",
      "systematic approach was designed to provide a comprehensive\n",
      "understanding of how these models perform in terms of equi-\n",
      "table treatment across different demographic groups, thereby\n",
      "contributing to the broader discourse on AI fairness and ethical\n",
      "considerations in machine learning.\n",
      "We utilized two benchmark datasets, CivilComments and\n",
      "COMPAS, which are widely recognized for their applicability\n",
      "in assessing fairness in machine learning models. The Civil-\n",
      "Comments dataset contains a collection of online comments\n",
      "that have been annotated for toxicity and identity labels, thus\n",
      "providing a rich and varied dataset to evaluate the capabil-\n",
      "ity of LLMs to detect and mitigate toxic language without\n",
      "introducing biases. This dataset includes a broad range of\n",
      "identity labels, such as gender, race, and religion, allowing\n",
      "for a nuanced analysis of how various demographic groups\n",
      "are treated by the models. The COMPAS dataset, on the other\n",
      "hand, comprises demographic information and recidivism data\n",
      "used to assess criminal justice risk scores, and has been\n",
      "central to numerous discussions regarding fairness in predic-\n",
      "tive algorithms. By incorporating both datasets, we aimed to\n",
      "examine fairness from multiple perspectives, including racial\n",
      "and gender biases, across distinct domains, ensuring a holistic\n",
      "evaluation of model behavior.\n",
      "The LLMs evaluated in this study included models from\n",
      "Meta, Mistral, NousResearch, and Qwen. We assessed these\n",
      "models using state-of-the-art fairness metrics such as Demo-\n",
      "graphic Parity, True Positive Rate (TPR) Parity, and False\n",
      "Positive Rate (FPR) Parity. Demographic Parity ensures that\n",
      "the likelihood of positive predictions is equal across different\n",
      "demographic groups, indicating that no group is disproportion-\n",
      "ately favored or disadvantaged. TPR Parity evaluates whether\n",
      "true positive rates are consistent across demographic groups,\n",
      "ensuring that the model is equally effective in identifying\n",
      "positive cases irrespective of group membership. FPR Parity\n",
      "measures whether false positive rates are balanced across\n",
      "groups, which is critical for minimizing unjustified negative\n",
      "outcomes for certain populations. Together, these metrics\n",
      "provide a robust framework for identifying and quantifying\n",
      "biases in model predictions. To systematically calculate these\n",
      "metrics and identify disparities, we employed Aequitas, a well-\n",
      "established fairness assessment framework. Aequitas offers\n",
      "detailed breakdowns of fairness metrics and facilitates the\n",
      "comparison of model behaviors across multiple demographic\n",
      "attributes, making it highly suitable for this type of analysis.\n",
      "Our assessment involved a thorough examination of model\n",
      "performance across various identity groups, defined by at-\n",
      "tributes such as race, gender, and age, among others. By\n",
      "comparing the distribution of model outputs and prediction\n",
      "patterns across different groups, we were able to identify\n",
      "specific areas in which the models demonstrated biases. For\n",
      "instance, discrepancies in TPR or FPR between groups may in-\n",
      "dicate that the model is systematically more likely to correctly\n",
      "or incorrectly classify instances from certain demographics,\n",
      "which has profound implications for fairness in practical, real-\n",
      "world applications. The inclusion of multiple datasets also\n",
      "enabled us to assess the consistency of these biases across\n",
      "different contexts, providing insights into whether these biases\n",
      "are inherent to the model architecture or are induced by the\n",
      "nature of the training data.\n",
      "Overall, our methodology was not solely focused on iden-\n",
      "tifying the presence of biases but also on uncovering their\n",
      "root causes. By employing comprehensive fairness metrics and\n",
      "leveraging well-established benchmark datasets, we aimed togenerate a clear and detailed understanding of how LLMs\n",
      "from different organizations perform with respect to fairness\n",
      "and equity. These evaluations are crucial for guiding future\n",
      "improvements in model training and fine-tuning processes,\n",
      "ensuring that LLMs can be deployed responsibly and ethically,\n",
      "particularly in high-stakes applications where fairness is of\n",
      "utmost importance.\n",
      "A. Experimental Setup A\n",
      "For the evaluation of the CivilComments dataset, we estab-\n",
      "lished a rigorous experimental setup designed to systematically\n",
      "assess the fairness and predictive performance of the selected\n",
      "large language models (LLMs). The CivilComments dataset,\n",
      "consisting of online comments annotated for toxicity and\n",
      "various identity labels, was preprocessed and prepared for\n",
      "model evaluation to ensure consistency and comparability of\n",
      "results.\n",
      "We began by selecting a subset of 5,000 comments from\n",
      "the test set of the CivilComments dataset to ensure the feasi-\n",
      "bility of the evaluation while maintaining statistical power.\n",
      "The comments were tokenized using the appropriate tok-\n",
      "enizer for each model, ensuring compatibility and optimal\n",
      "input representation. To address the challenge of missing or\n",
      "inconsistent tokens, we assigned the padding token to the\n",
      "end-of-sequence token wherever necessary. The preprocessing\n",
      "included truncation to a maximum length of 128 tokens to\n",
      "manage computational resources effectively while preserving\n",
      "key contextual information.\n",
      "Each model was configured for binary classification to dis-\n",
      "tinguish between toxic and non-toxic comments. The models\n",
      "were loaded with a consistent configuration, including the\n",
      "setting of two output labels to represent the binary nature of\n",
      "the task. During the evaluation, we employed a batch size of\n",
      "256, enabling efficient utilization of available computational\n",
      "resources while ensuring stable convergence of model predic-\n",
      "tions.\n",
      "The evaluation process utilized a DataLoader with a data\n",
      "collator that performed dynamic padding, ensuring that input\n",
      "sequences were appropriately aligned for each batch. The\n",
      "models were run in inference mode to generate predictions for\n",
      "each comment, and key metrics such as True Positive Rate\n",
      "(TPR), False Positive Rate (FPR), and demographic parity\n",
      "were calculated to assess model performance across different\n",
      "demographic groups. This step allowed us to capture both\n",
      "aggregate performance metrics and any disparities in outcomes\n",
      "based on sensitive attributes such as race, gender, or other\n",
      "identity labels.\n",
      "Overall, the experimental setup for the CivilComments\n",
      "dataset was designed to provide a comprehensive evaluation\n",
      "of fairness and bias across different LLMs, with particular em-\n",
      "phasis on the consistency and comparability of results across\n",
      "demographic groups. This setup provided critical insights into\n",
      "how each model handles toxic content and the extent to which\n",
      "biases may be present in model predictions, highlighting areas\n",
      "for potential improvement in fairness and equity.\n",
      "B. Experimental Setup B\n",
      "For the evaluation of the CivilComments dataset, we es-\n",
      "tablished an experimental setup to assess the fairness and\n",
      "predictive performance of large language models (LLMs).\n",
      "The CivilComments dataset, consisting of online comments\n",
      "annotated for toxicity and identity labels, was preprocessed to\n",
      "ensure consistency in model evaluation.\n",
      "We selected 5,000 comments from the test set for evaluation.\n",
      "Comments were tokenized with appropriate tokenizers for each\n",
      "model, and truncation was applied to a maximum length of 128\n",
      "tokens to balance computational efficiency and information\n",
      "retention. Padding tokens were added where necessary to\n",
      "maintain input consistency.\n",
      "Each model was configured for binary classification to\n",
      "distinguish toxic from non-toxic comments. A batch size of\n",
      "256 was used for efficient resource utilization. The evaluation\n",
      "employed a DataLoader with dynamic padding to align input\n",
      "sequences and generate predictions, which were then used\n",
      "to calculate metrics such as True Positive Rate (TPR), False\n",
      "Positive Rate (FPR), and demographic parity to assess fairness\n",
      "across demographic groups.\n",
      "Overall, this setup provided insights into model performance\n",
      "on toxic content and highlighted areas for improvement in\n",
      "fairness and equity.\n",
      "For the COMPAS dataset, we designed an experimental\n",
      "setup to evaluate LLMs’ predictive performance and fairness\n",
      "in criminal justice risk assessment. The dataset, which contains\n",
      "demographic information and recidivism data, was used to\n",
      "analyze the models’ ability to predict recidivism without\n",
      "significant demographic bias.\n",
      "We used the entire training split of COMPAS, including de-\n",
      "mographic features like race, gender, and age, and recidivism\n",
      "labels. Data preprocessing involved converting categorical\n",
      "attributes into suitable numerical formats. Each model was\n",
      "configured for binary classification of recidivism, with a batch\n",
      "size of 256 for consistency.\n",
      "Metrics such as Demographic Parity, TPR Parity, FPR\n",
      "Parity, and False Discovery Rate (FDR) Parity were computed\n",
      "to assess fairness across groups, focusing on race and gender.\n",
      "These metrics helped identify biases and evaluate fairness in\n",
      "model predictions.\n",
      "Overall, this setup provided a thorough evaluation of pre-\n",
      "dictive accuracy and fairness, highlighting areas where biases\n",
      "may arise and offering insights into improving fairness in AI\n",
      "systems for high-stakes applications.\n",
      "IV. E MPIRICAL EVALUATION\n",
      "The empirical evaluation involved running each model on\n",
      "the prepared datasets and analyzing the resulting predictions.\n",
      "We compared model performance using fairness metrics, such\n",
      "as demographic parity and TPR/FPR parity, to understand\n",
      "disparities in model behavior across different demographic\n",
      "groups. This analysis provided a detailed assessment of both\n",
      "model accuracy and fairness, offering valuable insights into\n",
      "potential biases and areas for improvement.A. CivilComments\n",
      "TABLE I\n",
      "TOTAL NUMBER OF OBSERVATIONS 25000\n",
      "Model Demographic Parity TPR FPR FDR A VG\n",
      "Llama 3.1 8B 0.35 0.40 0.45 0.50 0.43\n",
      "Llama 3.1 70B 0.30 0.35 0.40 0.45 0.38\n",
      "Llama 3.2 1B 0.28 0.32 0.38 0.42 0.35\n",
      "Llama 3.2 3B 0.25 0.30 0.34 0.40 0.32\n",
      "Mistral 7B 0.55 0.60 0.65 0.70 0.63\n",
      "Mixtral 8x7B 0.95 1.00 1.05 1.10 1.03\n",
      "Hermes 3 8B 0.15 0.18 0.20 0.22 0.19\n",
      "Hermes 3 70B 0.12 0.14 0.16 0.18 0.15\n",
      "Qwen 2.5 1.5B 0.45 0.50 0.55 0.60 0.53\n",
      "Qwen 2.5 7B 0.50 0.55 0.58 0.62 0.56\n",
      "Qwen 2.5 72B 0.48 0.52 0.56 0.60 0.54\n",
      "The results indicate that Hermes models (Hermes 3 8B and\n",
      "Hermes 3 70B) are the least biased, with average bias values\n",
      "of 0.19 and 0.15 respectively, suggesting high fairness across\n",
      "the metrics. The Llama models also exhibit relatively low bias,\n",
      "especially the Llama 3.2 versions, with averages ranging from\n",
      "0.32 to 0.35.\n",
      "On the other hand, Mixtral 8x7B has the highest bias across\n",
      "all metrics, with an average of 1.03, indicating significant\n",
      "fairness concerns. The Qwen models show moderate bias, with\n",
      "averages between 0.53 and 0.56, making them more biased\n",
      "compared to Hermes and Llama, but still better than Mixtral.\n",
      "Overall, the Hermes models stand out as the most fair, while\n",
      "Mixtral requires significant improvements to reduce bias.\n",
      "B. COMPAS\n",
      "TABLE II\n",
      "TOTAL NUMBER OF OBSERVATIONS 4353\n",
      "Model Demographic Parity TPR FPR A VG\n",
      "Llama 3.1 8B 0.40 0.45 0.50 0.45\n",
      "Llama 3.1 70B 0.35 0.40 0.42 0.39\n",
      "Llama 3.2 1B 0.32 0.38 0.40 0.37\n",
      "Llama 3.2 3B 0.30 0.35 0.38 0.34\n",
      "Mistral 7B 0.60 0.65 0.68 0.64\n",
      "Mixtral 8x7B 1.00 1.05 1.10 1.05\n",
      "Hermes 3 8B 0.20 0.22 0.25 0.22\n",
      "Hermes 3 70B 0.18 0.20 0.22 0.20\n",
      "Qwen 2.5 1.5B 0.50 0.55 0.58 0.54\n",
      "Qwen 2.5 7B 0.52 0.56 0.60 0.56\n",
      "Qwen 2.5 72B 0.48 0.52 0.55 0.52\n",
      "The results from this dataset, focusing on racial bias, show\n",
      "a similar trend to previous analyses. The Hermes models\n",
      "(Hermes 3 8B and Hermes 3 70B) again display the least bias,\n",
      "with average values of 0.22 and 0.20, respectively, indicating\n",
      "a high level of fairness. The Llama models also demonstrate\n",
      "relatively low bias, especially Llama 3.2 models, with averages\n",
      "ranging from 0.34 to 0.37.\n",
      "Mixtral 8x7B exhibits the highest bias in this analysis as\n",
      "well, with an average of 1.05, suggesting significant fairness\n",
      "issues. The Qwen models show moderate bias, with averages\n",
      "between 0.52 and 0.56, making them more biased compared\n",
      "to Hermes and Llama but still less biased than Mixtral.\n",
      "Overall, Hermes models continue to be the most fair, while\n",
      "Mixtral models show substantial room for improvement in\n",
      "reducing racial bias.\n",
      "V. C ONCLUSION\n",
      "In this study, we conducted a comprehensive evaluation of\n",
      "fairness and bias in large language models (LLMs) developed\n",
      "by Meta, Mistral, NousResearch, and Qwen. Using benchmark\n",
      "datasets such as CivilComments and COMPAS, we applied\n",
      "advanced fairness metrics, including Demographic Parity, TPR\n",
      "Parity, FPR Parity, CUAE, and FDR Parity, to assess the\n",
      "models’ behavior across different demographic groups. The\n",
      "results demonstrated significant disparities in fairness across\n",
      "the evaluated models, with Hermes models consistently outper-\n",
      "forming others in terms of equitable predictions. Our findings\n",
      "highlight the importance of employing robust fairness evalu-\n",
      "ation frameworks and datasets to identify and mitigate biases\n",
      "in LLMs, particularly in high-stakes applications where biased\n",
      "outcomes can lead to serious social and ethical consequences.\n",
      "Future work should focus on refining mitigation techniques\n",
      "and developing more transparent, interpretable models that\n",
      "can better address fairness concerns. Collaboration between\n",
      "researchers, policymakers, and industry stakeholders is crucial\n",
      "to ensure that AI systems are deployed in ways that promote\n",
      "fairness, equity, and inclusivity.\n",
      "ACKNOWLEDGMENT\n",
      "The authors would like to thank the developers and re-\n",
      "search teams at Meta, Mistral, NousResearch, and Qwen for\n",
      "providing access to their models for evaluation. We also\n",
      "acknowledge the support of the Aequitas framework team for\n",
      "their invaluable assistance in applying fairness metrics.\n",
      "REFERENCES\n",
      "1) J. Zhao, M. Fang, S. Pan, W. Yin, and M. Pechenizkiy, ”GPT-\n",
      "BIAS: A Comprehensive Framework for Evaluating Bias in\n",
      "Large Language Models,” arXiv preprint arXiv:2312.06315v1 ,\n",
      "Dec. 2023.\n",
      "2) D. Bouchard, ”An Actionable Framework for Assessing Bias and\n",
      "Fairness in Large Language Model Use Cases,” arXiv preprint\n",
      "arXiv:2407.10853v2 , Aug. 2024.\n",
      "3) A. F. Oketunji, M. Anas, and D. Saina, ”Large Language Model\n",
      "(LLM) Bias Index—LLMBI,” arXiv preprint arXiv:2312.14769v3 ,\n",
      "Dec. 2023.\n",
      "4) W. Jeung, D. Jeon, A. Yousefpour, and J. Choi, ”Large Lan-\n",
      "guage Models Still Exhibit Bias in Long Text,” arXiv preprint\n",
      "arXiv:2410.17519v2 , Oct. 2024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "with open(file_path, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Initialize a variable to hold the combined text\n",
    "    combined_text = \"\"\n",
    "    \n",
    "    # Iterate through all the pages\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        # Get a specific page\n",
    "        page = reader.pages[page_num]\n",
    "        \n",
    "        # Extract text from the page\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        # Add the text to the combined text variable\n",
    "        combined_text += text + \"\\n\"  # Adding a newline for separation between pages\n",
    "\n",
    "# Print the combined text\n",
    "print(combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amish\\AppData\\Local\\Temp\\ipykernel_16364\\1026163094.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response=generate_evaluate_chain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Evaluating Fairness and Bias in Large Language\n",
      "Models: A Comparative Analysis\n",
      "1stGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID2ndGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID3rdGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID\n",
      "4thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID5thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID6thGiven Name Surname\n",
      "dept. name of organization (of Aff.)\n",
      "name of organization (of Aff.)\n",
      "City, Country\n",
      "email address or ORCID\n",
      "Abstract —The rapid development and deployment of large lan-\n",
      "guage models (LLMs) have transformed various fields, including\n",
      "natural language processing, healthcare, and criminal justice.\n",
      "However, these models have raised significant concerns regarding\n",
      "fairness, equity, and bias, particularly in high-stakes applications\n",
      "where biased predictions can have adverse consequences. This\n",
      "study systematically evaluates the fairness and bias of LLMs\n",
      "developed by prominent organizations such as Meta, Mistral,\n",
      "NousResearch, and Qwen. Using benchmark datasets such as\n",
      "CivilComments and COMPAS, and employing advanced fairness\n",
      "metrics, we assess models’ predictive performance and biases\n",
      "across demographic groups. Metrics including Demographic\n",
      "Parity, True Positive Rate (TPR) Parity, False Positive Rate (FPR)\n",
      "Parity, Conditional Use Accuracy Equality (CUAE), and False\n",
      "Discovery Rate (FDR) Parity are used to quantify disparities in\n",
      "model predictions. The empirical results reveal varying degrees\n",
      "of bias among the LLMs, with Hermes models consistently\n",
      "demonstrating higher fairness compared to other evaluated\n",
      "models. This research provides critical insights into the fairness\n",
      "of LLMs, underscoring the need for responsible development\n",
      "and deployment of AI systems to mitigate potential biases and\n",
      "promote equitable outcomes.\n",
      "Index Terms —Fairness, Bias, Large Language Models, Civil-\n",
      "Comments, COMPAS, Demographic Parity, True Positive Rate\n",
      "Parity, False Positive Rate Parity, Machine Learning, Ethical AI,\n",
      "Aequitas.\n",
      "I. I NTRODUCTION\n",
      "The development and deployment of Machine Learning\n",
      "(ML) and Artificial Intelligence (AI) systems in high-stakes\n",
      "applications, such as criminal justice, healthcare, and hiring,\n",
      "have raised pressing concerns about fairness, equity, and\n",
      "bias. As these systems increasingly impact decisions affecting\n",
      "individuals and communities, mitigating biases related to race,\n",
      "gender, age, and other sensitive attributes has become essen-\n",
      "tial. The presence of unfair biases in these models can perpet-\n",
      "uate systemic inequalities, leading to adverse consequences\n",
      "such as racial discrimination, sexism, and ageism.\n",
      "Fairness in ML models is often assessed through metrics\n",
      "that evaluate the treatment and outcomes for different demo-graphic groups. For instance, demographic parity, also known\n",
      "as statistical parity, is a fairness criterion that requires the\n",
      "probability of a positive prediction to be equal across groups:\n",
      "P(ˆY= 1 |A=a) =P(ˆY= 1 |A=b)\n",
      "where A represents the sensitive attribute,such as race or\n",
      "gender.\n",
      "When this criterion is not met, it indicates potential bias\n",
      "in the model’s predictions for certain groups. In this study,\n",
      "we evaluate large language models (LLMs) from leading re-\n",
      "search organizations, including Meta, Mistral, NousResearch,\n",
      "and Qwen, using state-of-the-art fairness and bias assessment\n",
      "frameworks. By applying these frameworks, we aim to quan-\n",
      "tify the extent of fairness and bias in model predictions. We\n",
      "assess metrics such as True Positive Rate (TPR) Parity and\n",
      "False Positive Rate (FPR) Parity, which evaluate whether the\n",
      "rates of correct positive predictions and incorrect positive\n",
      "predictions are consistent across groups:\n",
      "TPR Parity:P(ˆY= 1 |Y= 1, A=a)\n",
      "P(ˆY= 1 |Y= 1, A=b)= 1 (1)\n",
      "FPR Parity:P(ˆY= 1 |Y= 0, A=a)\n",
      "P(ˆY= 1 |Y= 0, A=b)= 1 (2)\n",
      "where Y is the true label and ˆYis the predicted label.\n",
      "By examining Conditional Use Accuracy Equality (CUAE)\n",
      "and False Discovery Rate (FDR) Parity, we further explore\n",
      "how accurately the model’s positive predictions reflect the\n",
      "ground truth across demographic groups, highlighting potential\n",
      "disparities in the model’s decision boundaries:\n",
      "CUAE:P(Y= 1 |ˆY= 1, A=a)\n",
      "P(Y= 1 |ˆY= 1, A=b)= 1 (3)\n",
      "FDR Parity:P(Y= 0 |ˆY= 1, A=a)\n",
      "P(Y= 0 |ˆY= 1, A=b)= 1 (4)\n",
      "Our research leverages advanced fairness frameworks to\n",
      "systematically evaluate these metrics across datasets used in\n",
      "training and fine-tuning each model. By assessing LLMs from\n",
      "Meta, Mistral, NousResearch, and Qwen, we provide insights\n",
      "into the fairness of these models, the sources of potential\n",
      "bias in their training datasets, and the broader implications\n",
      "for equitable AI.\n",
      "II. B ACKGROUND AND RELATED WORK\n",
      "The rapid advancement of large language models (LLMs)\n",
      "has revolutionized natural language processing, providing high\n",
      "accuracy across various tasks such as text generation, sen-\n",
      "timent analysis, and classification. These models, including\n",
      "OpenAI’s GPT series, Meta’s LLaMa, and others, are exten-\n",
      "sively used across industries, but their deployment has raised\n",
      "significant ethical concerns. Specifically, LLMs are prone to\n",
      "inherit and amplify biases present in the large-scale datasets\n",
      "on which they are trained. Bias in LLMs reflects societal\n",
      "and historical inequalities, manifesting across multiple demo-\n",
      "graphic attributes such as gender, race, age, and socioeconomic\n",
      "status. Addressing these biases is crucial, as biased models\n",
      "can lead to unfair or harmful outcomes, especially in sensitive\n",
      "applications(e.g., healthcare, hiring, and law enforcement).\n",
      "1) Bias and Fairness Taxonomies in LLMs: Several studies\n",
      "have attempted to formalize and categorize bias and fairness\n",
      "in LLMs. Gallegos et al. (2023) consolidated definitions of\n",
      "social bias and fairness specifically for NLP tasks, proposing\n",
      "three taxonomies for bias evaluation: metrics, datasets, and\n",
      "mitigation techniques. The authors argue that fairness should\n",
      "be defined not only through quantitative metrics but also by\n",
      "considering the broader social impacts of model outputs. Their\n",
      "taxonomy organizes metrics by level, such as embedding-\n",
      "based, probability-based, and text-based metrics, allowing re-\n",
      "searchers to choose appropriate evaluation techniques based on\n",
      "access to model outputs. Similarly, Bouchard (2024) proposed\n",
      "an actionable framework for aligning LLM use cases with\n",
      "suitable bias and fairness metrics, focusing on context-specific\n",
      "risk assessment for LLM applications.\n",
      "2) Existing Frameworks for Bias Evaluation: Current meth-\n",
      "ods for evaluating bias in LLMs range from simple bench-\n",
      "marks to sophisticated frameworks. Zhao et al. introduced\n",
      "GPTBIAS , a framework that leverages advanced LLMs (such\n",
      "as GPT-4) for detecting specific bias types, including gen-\n",
      "der, race, and age biases. GPTBIAS uses specially designed\n",
      "prompts to expose biases in model responses, providing de-\n",
      "tailed information on bias types, affected demographics, and\n",
      "suggestions for mitigating bias. Additionally, Oketunji et al.\n",
      "developed the LLM Bias Index (LLMBI) , a metric designedto quantify bias in LLMs systematically across multiple di-\n",
      "mensions. LLMBI incorporates both demographic information\n",
      "and sentiment analysis to capture subtle biases, offering a\n",
      "composite score that reflects the extent of bias in various\n",
      "categories.\n",
      "3) Limitations of Existing Bias Metrics and Datasets:\n",
      "Many traditional bias evaluation techniques, such as word em-\n",
      "beddings or probability-based metrics, have limited robustness\n",
      "and interpretability when applied to complex scenarios, such\n",
      "as long-text generation. Jeung et al. introduced the Long Text\n",
      "Fairness Test (LTF-TEST) , which focuses on identifying\n",
      "biases that arise in extended text outputs. LTF-TEST evaluates\n",
      "model responses to paired prompts, allowing for a more\n",
      "nuanced assessment of bias in scenarios where simpler metrics\n",
      "may fall short. This approach highlights the importance of\n",
      "considering bias in long-form content, as biases can manifest\n",
      "differently compared to shorter or simpler tasks.\n",
      "4) Novel Approaches to Bias Mitigation: While numerous\n",
      "methods have been proposed to mitigate bias, their effective-\n",
      "ness varies based on the model and application. Techniques\n",
      "such as instruction fine-tuning and prompt engineering\n",
      "have shown promise in reducing biases by guiding the model\n",
      "towards more neutral responses. For example, REGARD-FT,\n",
      "a fine-tuning approach, pairs biased prompts with neutral\n",
      "responses, effectively reducing gender bias and improving\n",
      "model performance on fairness benchmarks. However, these\n",
      "techniques may still be limited in mitigating all forms of bias,\n",
      "especially when applied to proprietary or black-box models.\n",
      "5) Summary and Contributions: The existing literature re-\n",
      "veals substantial progress in bias evaluation and mitigation\n",
      "for LLMs, yet there remain significant challenges, particu-\n",
      "larly regarding interpretability, dataset alignment, and domain-\n",
      "specific application. Our research builds on these foundations\n",
      "by applying state-of-the-art bias and fairness frameworks to\n",
      "evaluate multiple open-source LLMs from various organiza-\n",
      "tions, including Meta, Mistral, NousResearch, and Qwen. Our\n",
      "study assesses these models across diverse datasets, examining\n",
      "how their training data and architecture influence bias. This\n",
      "work contributes to the ongoing efforts to develop fairer\n",
      "and more equitable AI systems, highlighting the nuances and\n",
      "limitations of current bias evaluation frameworks.\n",
      "III. M ETHODOLOGY\n",
      "In this study, we adopted a rigorous methodology to evaluate\n",
      "fairness and bias in large language models (LLMs) developed\n",
      "by multiple leading research organizations. Our evaluation\n",
      "framework consisted of two core components: dataset selection\n",
      "and model assessment using advanced fairness metrics. This\n",
      "systematic approach was designed to provide a comprehensive\n",
      "understanding of how these models perform in terms of equi-\n",
      "table treatment across different demographic groups, thereby\n",
      "contributing to the broader discourse on AI fairness and ethical\n",
      "considerations in machine learning.\n",
      "We utilized two benchmark datasets, CivilComments and\n",
      "COMPAS, which are widely recognized for their applicability\n",
      "in assessing fairness in machine learning models. The Civil-\n",
      "Comments dataset contains a collection of online comments\n",
      "that have been annotated for toxicity and identity labels, thus\n",
      "providing a rich and varied dataset to evaluate the capabil-\n",
      "ity of LLMs to detect and mitigate toxic language without\n",
      "introducing biases. This dataset includes a broad range of\n",
      "identity labels, such as gender, race, and religion, allowing\n",
      "for a nuanced analysis of how various demographic groups\n",
      "are treated by the models. The COMPAS dataset, on the other\n",
      "hand, comprises demographic information and recidivism data\n",
      "used to assess criminal justice risk scores, and has been\n",
      "central to numerous discussions regarding fairness in predic-\n",
      "tive algorithms. By incorporating both datasets, we aimed to\n",
      "examine fairness from multiple perspectives, including racial\n",
      "and gender biases, across distinct domains, ensuring a holistic\n",
      "evaluation of model behavior.\n",
      "The LLMs evaluated in this study included models from\n",
      "Meta, Mistral, NousResearch, and Qwen. We assessed these\n",
      "models using state-of-the-art fairness metrics such as Demo-\n",
      "graphic Parity, True Positive Rate (TPR) Parity, and False\n",
      "Positive Rate (FPR) Parity. Demographic Parity ensures that\n",
      "the likelihood of positive predictions is equal across different\n",
      "demographic groups, indicating that no group is disproportion-\n",
      "ately favored or disadvantaged. TPR Parity evaluates whether\n",
      "true positive rates are consistent across demographic groups,\n",
      "ensuring that the model is equally effective in identifying\n",
      "positive cases irrespective of group membership. FPR Parity\n",
      "measures whether false positive rates are balanced across\n",
      "groups, which is critical for minimizing unjustified negative\n",
      "outcomes for certain populations. Together, these metrics\n",
      "provide a robust framework for identifying and quantifying\n",
      "biases in model predictions. To systematically calculate these\n",
      "metrics and identify disparities, we employed Aequitas, a well-\n",
      "established fairness assessment framework. Aequitas offers\n",
      "detailed breakdowns of fairness metrics and facilitates the\n",
      "comparison of model behaviors across multiple demographic\n",
      "attributes, making it highly suitable for this type of analysis.\n",
      "Our assessment involved a thorough examination of model\n",
      "performance across various identity groups, defined by at-\n",
      "tributes such as race, gender, and age, among others. By\n",
      "comparing the distribution of model outputs and prediction\n",
      "patterns across different groups, we were able to identify\n",
      "specific areas in which the models demonstrated biases. For\n",
      "instance, discrepancies in TPR or FPR between groups may in-\n",
      "dicate that the model is systematically more likely to correctly\n",
      "or incorrectly classify instances from certain demographics,\n",
      "which has profound implications for fairness in practical, real-\n",
      "world applications. The inclusion of multiple datasets also\n",
      "enabled us to assess the consistency of these biases across\n",
      "different contexts, providing insights into whether these biases\n",
      "are inherent to the model architecture or are induced by the\n",
      "nature of the training data.\n",
      "Overall, our methodology was not solely focused on iden-\n",
      "tifying the presence of biases but also on uncovering their\n",
      "root causes. By employing comprehensive fairness metrics and\n",
      "leveraging well-established benchmark datasets, we aimed togenerate a clear and detailed understanding of how LLMs\n",
      "from different organizations perform with respect to fairness\n",
      "and equity. These evaluations are crucial for guiding future\n",
      "improvements in model training and fine-tuning processes,\n",
      "ensuring that LLMs can be deployed responsibly and ethically,\n",
      "particularly in high-stakes applications where fairness is of\n",
      "utmost importance.\n",
      "A. Experimental Setup A\n",
      "For the evaluation of the CivilComments dataset, we estab-\n",
      "lished a rigorous experimental setup designed to systematically\n",
      "assess the fairness and predictive performance of the selected\n",
      "large language models (LLMs). The CivilComments dataset,\n",
      "consisting of online comments annotated for toxicity and\n",
      "various identity labels, was preprocessed and prepared for\n",
      "model evaluation to ensure consistency and comparability of\n",
      "results.\n",
      "We began by selecting a subset of 5,000 comments from\n",
      "the test set of the CivilComments dataset to ensure the feasi-\n",
      "bility of the evaluation while maintaining statistical power.\n",
      "The comments were tokenized using the appropriate tok-\n",
      "enizer for each model, ensuring compatibility and optimal\n",
      "input representation. To address the challenge of missing or\n",
      "inconsistent tokens, we assigned the padding token to the\n",
      "end-of-sequence token wherever necessary. The preprocessing\n",
      "included truncation to a maximum length of 128 tokens to\n",
      "manage computational resources effectively while preserving\n",
      "key contextual information.\n",
      "Each model was configured for binary classification to dis-\n",
      "tinguish between toxic and non-toxic comments. The models\n",
      "were loaded with a consistent configuration, including the\n",
      "setting of two output labels to represent the binary nature of\n",
      "the task. During the evaluation, we employed a batch size of\n",
      "256, enabling efficient utilization of available computational\n",
      "resources while ensuring stable convergence of model predic-\n",
      "tions.\n",
      "The evaluation process utilized a DataLoader with a data\n",
      "collator that performed dynamic padding, ensuring that input\n",
      "sequences were appropriately aligned for each batch. The\n",
      "models were run in inference mode to generate predictions for\n",
      "each comment, and key metrics such as True Positive Rate\n",
      "(TPR), False Positive Rate (FPR), and demographic parity\n",
      "were calculated to assess model performance across different\n",
      "demographic groups. This step allowed us to capture both\n",
      "aggregate performance metrics and any disparities in outcomes\n",
      "based on sensitive attributes such as race, gender, or other\n",
      "identity labels.\n",
      "Overall, the experimental setup for the CivilComments\n",
      "dataset was designed to provide a comprehensive evaluation\n",
      "of fairness and bias across different LLMs, with particular em-\n",
      "phasis on the consistency and comparability of results across\n",
      "demographic groups. This setup provided critical insights into\n",
      "how each model handles toxic content and the extent to which\n",
      "biases may be present in model predictions, highlighting areas\n",
      "for potential improvement in fairness and equity.\n",
      "B. Experimental Setup B\n",
      "For the evaluation of the CivilComments dataset, we es-\n",
      "tablished an experimental setup to assess the fairness and\n",
      "predictive performance of large language models (LLMs).\n",
      "The CivilComments dataset, consisting of online comments\n",
      "annotated for toxicity and identity labels, was preprocessed to\n",
      "ensure consistency in model evaluation.\n",
      "We selected 5,000 comments from the test set for evaluation.\n",
      "Comments were tokenized with appropriate tokenizers for each\n",
      "model, and truncation was applied to a maximum length of 128\n",
      "tokens to balance computational efficiency and information\n",
      "retention. Padding tokens were added where necessary to\n",
      "maintain input consistency.\n",
      "Each model was configured for binary classification to\n",
      "distinguish toxic from non-toxic comments. A batch size of\n",
      "256 was used for efficient resource utilization. The evaluation\n",
      "employed a DataLoader with dynamic padding to align input\n",
      "sequences and generate predictions, which were then used\n",
      "to calculate metrics such as True Positive Rate (TPR), False\n",
      "Positive Rate (FPR), and demographic parity to assess fairness\n",
      "across demographic groups.\n",
      "Overall, this setup provided insights into model performance\n",
      "on toxic content and highlighted areas for improvement in\n",
      "fairness and equity.\n",
      "For the COMPAS dataset, we designed an experimental\n",
      "setup to evaluate LLMs’ predictive performance and fairness\n",
      "in criminal justice risk assessment. The dataset, which contains\n",
      "demographic information and recidivism data, was used to\n",
      "analyze the models’ ability to predict recidivism without\n",
      "significant demographic bias.\n",
      "We used the entire training split of COMPAS, including de-\n",
      "mographic features like race, gender, and age, and recidivism\n",
      "labels. Data preprocessing involved converting categorical\n",
      "attributes into suitable numerical formats. Each model was\n",
      "configured for binary classification of recidivism, with a batch\n",
      "size of 256 for consistency.\n",
      "Metrics such as Demographic Parity, TPR Parity, FPR\n",
      "Parity, and False Discovery Rate (FDR) Parity were computed\n",
      "to assess fairness across groups, focusing on race and gender.\n",
      "These metrics helped identify biases and evaluate fairness in\n",
      "model predictions.\n",
      "Overall, this setup provided a thorough evaluation of pre-\n",
      "dictive accuracy and fairness, highlighting areas where biases\n",
      "may arise and offering insights into improving fairness in AI\n",
      "systems for high-stakes applications.\n",
      "IV. E MPIRICAL EVALUATION\n",
      "The empirical evaluation involved running each model on\n",
      "the prepared datasets and analyzing the resulting predictions.\n",
      "We compared model performance using fairness metrics, such\n",
      "as demographic parity and TPR/FPR parity, to understand\n",
      "disparities in model behavior across different demographic\n",
      "groups. This analysis provided a detailed assessment of both\n",
      "model accuracy and fairness, offering valuable insights into\n",
      "potential biases and areas for improvement.A. CivilComments\n",
      "TABLE I\n",
      "TOTAL NUMBER OF OBSERVATIONS 25000\n",
      "Model Demographic Parity TPR FPR FDR A VG\n",
      "Llama 3.1 8B 0.35 0.40 0.45 0.50 0.43\n",
      "Llama 3.1 70B 0.30 0.35 0.40 0.45 0.38\n",
      "Llama 3.2 1B 0.28 0.32 0.38 0.42 0.35\n",
      "Llama 3.2 3B 0.25 0.30 0.34 0.40 0.32\n",
      "Mistral 7B 0.55 0.60 0.65 0.70 0.63\n",
      "Mixtral 8x7B 0.95 1.00 1.05 1.10 1.03\n",
      "Hermes 3 8B 0.15 0.18 0.20 0.22 0.19\n",
      "Hermes 3 70B 0.12 0.14 0.16 0.18 0.15\n",
      "Qwen 2.5 1.5B 0.45 0.50 0.55 0.60 0.53\n",
      "Qwen 2.5 7B 0.50 0.55 0.58 0.62 0.56\n",
      "Qwen 2.5 72B 0.48 0.52 0.56 0.60 0.54\n",
      "The results indicate that Hermes models (Hermes 3 8B and\n",
      "Hermes 3 70B) are the least biased, with average bias values\n",
      "of 0.19 and 0.15 respectively, suggesting high fairness across\n",
      "the metrics. The Llama models also exhibit relatively low bias,\n",
      "especially the Llama 3.2 versions, with averages ranging from\n",
      "0.32 to 0.35.\n",
      "On the other hand, Mixtral 8x7B has the highest bias across\n",
      "all metrics, with an average of 1.03, indicating significant\n",
      "fairness concerns. The Qwen models show moderate bias, with\n",
      "averages between 0.53 and 0.56, making them more biased\n",
      "compared to Hermes and Llama, but still better than Mixtral.\n",
      "Overall, the Hermes models stand out as the most fair, while\n",
      "Mixtral requires significant improvements to reduce bias.\n",
      "B. COMPAS\n",
      "TABLE II\n",
      "TOTAL NUMBER OF OBSERVATIONS 4353\n",
      "Model Demographic Parity TPR FPR A VG\n",
      "Llama 3.1 8B 0.40 0.45 0.50 0.45\n",
      "Llama 3.1 70B 0.35 0.40 0.42 0.39\n",
      "Llama 3.2 1B 0.32 0.38 0.40 0.37\n",
      "Llama 3.2 3B 0.30 0.35 0.38 0.34\n",
      "Mistral 7B 0.60 0.65 0.68 0.64\n",
      "Mixtral 8x7B 1.00 1.05 1.10 1.05\n",
      "Hermes 3 8B 0.20 0.22 0.25 0.22\n",
      "Hermes 3 70B 0.18 0.20 0.22 0.20\n",
      "Qwen 2.5 1.5B 0.50 0.55 0.58 0.54\n",
      "Qwen 2.5 7B 0.52 0.56 0.60 0.56\n",
      "Qwen 2.5 72B 0.48 0.52 0.55 0.52\n",
      "The results from this dataset, focusing on racial bias, show\n",
      "a similar trend to previous analyses. The Hermes models\n",
      "(Hermes 3 8B and Hermes 3 70B) again display the least bias,\n",
      "with average values of 0.22 and 0.20, respectively, indicating\n",
      "a high level of fairness. The Llama models also demonstrate\n",
      "relatively low bias, especially Llama 3.2 models, with averages\n",
      "ranging from 0.34 to 0.37.\n",
      "Mixtral 8x7B exhibits the highest bias in this analysis as\n",
      "well, with an average of 1.05, suggesting significant fairness\n",
      "issues. The Qwen models show moderate bias, with averages\n",
      "between 0.52 and 0.56, making them more biased compared\n",
      "to Hermes and Llama but still less biased than Mixtral.\n",
      "Overall, Hermes models continue to be the most fair, while\n",
      "Mixtral models show substantial room for improvement in\n",
      "reducing racial bias.\n",
      "V. C ONCLUSION\n",
      "In this study, we conducted a comprehensive evaluation of\n",
      "fairness and bias in large language models (LLMs) developed\n",
      "by Meta, Mistral, NousResearch, and Qwen. Using benchmark\n",
      "datasets such as CivilComments and COMPAS, we applied\n",
      "advanced fairness metrics, including Demographic Parity, TPR\n",
      "Parity, FPR Parity, CUAE, and FDR Parity, to assess the\n",
      "models’ behavior across different demographic groups. The\n",
      "results demonstrated significant disparities in fairness across\n",
      "the evaluated models, with Hermes models consistently outper-\n",
      "forming others in terms of equitable predictions. Our findings\n",
      "highlight the importance of employing robust fairness evalu-\n",
      "ation frameworks and datasets to identify and mitigate biases\n",
      "in LLMs, particularly in high-stakes applications where biased\n",
      "outcomes can lead to serious social and ethical consequences.\n",
      "Future work should focus on refining mitigation techniques\n",
      "and developing more transparent, interpretable models that\n",
      "can better address fairness concerns. Collaboration between\n",
      "researchers, policymakers, and industry stakeholders is crucial\n",
      "to ensure that AI systems are deployed in ways that promote\n",
      "fairness, equity, and inclusivity.\n",
      "ACKNOWLEDGMENT\n",
      "The authors would like to thank the developers and re-\n",
      "search teams at Meta, Mistral, NousResearch, and Qwen for\n",
      "providing access to their models for evaluation. We also\n",
      "acknowledge the support of the Aequitas framework team for\n",
      "their invaluable assistance in applying fairness metrics.\n",
      "REFERENCES\n",
      "1) J. Zhao, M. Fang, S. Pan, W. Yin, and M. Pechenizkiy, ”GPT-\n",
      "BIAS: A Comprehensive Framework for Evaluating Bias in\n",
      "Large Language Models,” arXiv preprint arXiv:2312.06315v1 ,\n",
      "Dec. 2023.\n",
      "2) D. Bouchard, ”An Actionable Framework for Assessing Bias and\n",
      "Fairness in Large Language Model Use Cases,” arXiv preprint\n",
      "arXiv:2407.10853v2 , Aug. 2024.\n",
      "3) A. F. Oketunji, M. Anas, and D. Saina, ”Large Language Model\n",
      "(LLM) Bias Index—LLMBI,” arXiv preprint arXiv:2312.14769v3 ,\n",
      "Dec. 2023.\n",
      "4) W. Jeung, D. Jeon, A. Yousefpour, and J. Choi, ”Large Lan-\n",
      "guage Models Still Exhibit Bias in Long Text,” arXiv preprint\n",
      "arXiv:2410.17519v2 , Oct. 2024.\n",
      "\n",
      "You are an expert MCQ maker. Given bthe above text , it is your job tocreate a quiz of 5 multiple choice question for Artifical Intelligence students to Simple tone.\n",
      "Make sure the question are not repeated and check all the question to be confirming the text as well.\n",
      "Make sure to format your response like RESPONSE_JSON and use it as a guide.Ensure to make 5 MCQs\n",
      "###RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for Artifical Intelligence students.You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
      "if the quiz is not at per with the cognitive and analytical abilities of the students,update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
      "Quiz_MCQs:\n",
      "{\"1\": {\"mcq\": \"Which large language models were evaluated in the study for fairness and bias?\", \"options\": {\"a\": \"Meta, Mistral, NousResearch, and Qwen\", \"b\": \"Google, IBM, Amazon, and Microsoft\", \"c\": \"OpenAI, DeepMind, Facebook AI, and Apple AI\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"2\": {\"mcq\": \"Which benchmark datasets were used in the study?\", \"options\": {\"a\": \"IMDB and Yelp\", \"b\": \"Twitter and Facebook\", \"c\": \"CivilComments and COMPAS\", \"d\": \"None of the above\"}, \"response\": \"c\"}, \"3\": {\"mcq\": \"Which model consistently demonstrated higher fairness compared to others?\", \"options\": {\"a\": \"Hermes\", \"b\": \"Llama\", \"c\": \"Mistral\", \"d\": \"Qwen\"}, \"response\": \"a\"}, \"4\": {\"mcq\": \"What is the Demographic Parity metric used for?\", \"options\": {\"a\": \"To ensure that the likelihood of positive predictions is equal across different demographic groups\", \"b\": \"To evaluate whether true positive rates are consistent across demographic groups\", \"c\": \"To measure whether false positive rates are balanced across groups\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"5\": {\"mcq\": \"What does a high value in the fairness metrics indicate?\", \"options\": {\"a\": \"High bias\", \"b\": \"Low bias\", \"c\": \"No bias\", \"d\": \"None of the above\"}, \"response\": \"a\"}}\n",
      "\n",
      "Check from an expert English Writer of the above quiz:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            'text':combined_text,\n",
    "            'number':5,\n",
    "            'subject':'Artifical Intelligence',\n",
    "            'tone':\"Simple\",\n",
    "            'response_json':json.dumps(response_json)\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Token: 6393\n",
      "output Tokens: 731\n",
      "total tokens: 7124\n",
      "Total cost: 0.23564999999999997\n"
     ]
    }
   ],
   "source": [
    "print('Prompt Token:',cb.prompt_tokens)\n",
    "print('output Tokens:',cb.completion_tokens)\n",
    "print('total tokens:',cb.total_tokens)\n",
    "print('Total cost:',cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Evaluating Fairness and Bias in Large Language\\nModels: A Comparative Analysis\\n1stGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID2ndGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID3rdGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID\\n4thGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID5thGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID6thGiven Name Surname\\ndept. name of organization (of Aff.)\\nname of organization (of Aff.)\\nCity, Country\\nemail address or ORCID\\nAbstract —The rapid development and deployment of large lan-\\nguage models (LLMs) have transformed various fields, including\\nnatural language processing, healthcare, and criminal justice.\\nHowever, these models have raised significant concerns regarding\\nfairness, equity, and bias, particularly in high-stakes applications\\nwhere biased predictions can have adverse consequences. This\\nstudy systematically evaluates the fairness and bias of LLMs\\ndeveloped by prominent organizations such as Meta, Mistral,\\nNousResearch, and Qwen. Using benchmark datasets such as\\nCivilComments and COMPAS, and employing advanced fairness\\nmetrics, we assess models’ predictive performance and biases\\nacross demographic groups. Metrics including Demographic\\nParity, True Positive Rate (TPR) Parity, False Positive Rate (FPR)\\nParity, Conditional Use Accuracy Equality (CUAE), and False\\nDiscovery Rate (FDR) Parity are used to quantify disparities in\\nmodel predictions. The empirical results reveal varying degrees\\nof bias among the LLMs, with Hermes models consistently\\ndemonstrating higher fairness compared to other evaluated\\nmodels. This research provides critical insights into the fairness\\nof LLMs, underscoring the need for responsible development\\nand deployment of AI systems to mitigate potential biases and\\npromote equitable outcomes.\\nIndex Terms —Fairness, Bias, Large Language Models, Civil-\\nComments, COMPAS, Demographic Parity, True Positive Rate\\nParity, False Positive Rate Parity, Machine Learning, Ethical AI,\\nAequitas.\\nI. I NTRODUCTION\\nThe development and deployment of Machine Learning\\n(ML) and Artificial Intelligence (AI) systems in high-stakes\\napplications, such as criminal justice, healthcare, and hiring,\\nhave raised pressing concerns about fairness, equity, and\\nbias. As these systems increasingly impact decisions affecting\\nindividuals and communities, mitigating biases related to race,\\ngender, age, and other sensitive attributes has become essen-\\ntial. The presence of unfair biases in these models can perpet-\\nuate systemic inequalities, leading to adverse consequences\\nsuch as racial discrimination, sexism, and ageism.\\nFairness in ML models is often assessed through metrics\\nthat evaluate the treatment and outcomes for different demo-graphic groups. For instance, demographic parity, also known\\nas statistical parity, is a fairness criterion that requires the\\nprobability of a positive prediction to be equal across groups:\\nP(ˆY= 1 |A=a) =P(ˆY= 1 |A=b)\\nwhere A represents the sensitive attribute,such as race or\\ngender.\\nWhen this criterion is not met, it indicates potential bias\\nin the model’s predictions for certain groups. In this study,\\nwe evaluate large language models (LLMs) from leading re-\\nsearch organizations, including Meta, Mistral, NousResearch,\\nand Qwen, using state-of-the-art fairness and bias assessment\\nframeworks. By applying these frameworks, we aim to quan-\\ntify the extent of fairness and bias in model predictions. We\\nassess metrics such as True Positive Rate (TPR) Parity and\\nFalse Positive Rate (FPR) Parity, which evaluate whether the\\nrates of correct positive predictions and incorrect positive\\npredictions are consistent across groups:\\nTPR Parity:P(ˆY= 1 |Y= 1, A=a)\\nP(ˆY= 1 |Y= 1, A=b)= 1 (1)\\nFPR Parity:P(ˆY= 1 |Y= 0, A=a)\\nP(ˆY= 1 |Y= 0, A=b)= 1 (2)\\nwhere Y is the true label and ˆYis the predicted label.\\nBy examining Conditional Use Accuracy Equality (CUAE)\\nand False Discovery Rate (FDR) Parity, we further explore\\nhow accurately the model’s positive predictions reflect the\\nground truth across demographic groups, highlighting potential\\ndisparities in the model’s decision boundaries:\\nCUAE:P(Y= 1 |ˆY= 1, A=a)\\nP(Y= 1 |ˆY= 1, A=b)= 1 (3)\\nFDR Parity:P(Y= 0 |ˆY= 1, A=a)\\nP(Y= 0 |ˆY= 1, A=b)= 1 (4)\\nOur research leverages advanced fairness frameworks to\\nsystematically evaluate these metrics across datasets used in\\ntraining and fine-tuning each model. By assessing LLMs from\\nMeta, Mistral, NousResearch, and Qwen, we provide insights\\ninto the fairness of these models, the sources of potential\\nbias in their training datasets, and the broader implications\\nfor equitable AI.\\nII. B ACKGROUND AND RELATED WORK\\nThe rapid advancement of large language models (LLMs)\\nhas revolutionized natural language processing, providing high\\naccuracy across various tasks such as text generation, sen-\\ntiment analysis, and classification. These models, including\\nOpenAI’s GPT series, Meta’s LLaMa, and others, are exten-\\nsively used across industries, but their deployment has raised\\nsignificant ethical concerns. Specifically, LLMs are prone to\\ninherit and amplify biases present in the large-scale datasets\\non which they are trained. Bias in LLMs reflects societal\\nand historical inequalities, manifesting across multiple demo-\\ngraphic attributes such as gender, race, age, and socioeconomic\\nstatus. Addressing these biases is crucial, as biased models\\ncan lead to unfair or harmful outcomes, especially in sensitive\\napplications(e.g., healthcare, hiring, and law enforcement).\\n1) Bias and Fairness Taxonomies in LLMs: Several studies\\nhave attempted to formalize and categorize bias and fairness\\nin LLMs. Gallegos et al. (2023) consolidated definitions of\\nsocial bias and fairness specifically for NLP tasks, proposing\\nthree taxonomies for bias evaluation: metrics, datasets, and\\nmitigation techniques. The authors argue that fairness should\\nbe defined not only through quantitative metrics but also by\\nconsidering the broader social impacts of model outputs. Their\\ntaxonomy organizes metrics by level, such as embedding-\\nbased, probability-based, and text-based metrics, allowing re-\\nsearchers to choose appropriate evaluation techniques based on\\naccess to model outputs. Similarly, Bouchard (2024) proposed\\nan actionable framework for aligning LLM use cases with\\nsuitable bias and fairness metrics, focusing on context-specific\\nrisk assessment for LLM applications.\\n2) Existing Frameworks for Bias Evaluation: Current meth-\\nods for evaluating bias in LLMs range from simple bench-\\nmarks to sophisticated frameworks. Zhao et al. introduced\\nGPTBIAS , a framework that leverages advanced LLMs (such\\nas GPT-4) for detecting specific bias types, including gen-\\nder, race, and age biases. GPTBIAS uses specially designed\\nprompts to expose biases in model responses, providing de-\\ntailed information on bias types, affected demographics, and\\nsuggestions for mitigating bias. Additionally, Oketunji et al.\\ndeveloped the LLM Bias Index (LLMBI) , a metric designedto quantify bias in LLMs systematically across multiple di-\\nmensions. LLMBI incorporates both demographic information\\nand sentiment analysis to capture subtle biases, offering a\\ncomposite score that reflects the extent of bias in various\\ncategories.\\n3) Limitations of Existing Bias Metrics and Datasets:\\nMany traditional bias evaluation techniques, such as word em-\\nbeddings or probability-based metrics, have limited robustness\\nand interpretability when applied to complex scenarios, such\\nas long-text generation. Jeung et al. introduced the Long Text\\nFairness Test (LTF-TEST) , which focuses on identifying\\nbiases that arise in extended text outputs. LTF-TEST evaluates\\nmodel responses to paired prompts, allowing for a more\\nnuanced assessment of bias in scenarios where simpler metrics\\nmay fall short. This approach highlights the importance of\\nconsidering bias in long-form content, as biases can manifest\\ndifferently compared to shorter or simpler tasks.\\n4) Novel Approaches to Bias Mitigation: While numerous\\nmethods have been proposed to mitigate bias, their effective-\\nness varies based on the model and application. Techniques\\nsuch as instruction fine-tuning and prompt engineering\\nhave shown promise in reducing biases by guiding the model\\ntowards more neutral responses. For example, REGARD-FT,\\na fine-tuning approach, pairs biased prompts with neutral\\nresponses, effectively reducing gender bias and improving\\nmodel performance on fairness benchmarks. However, these\\ntechniques may still be limited in mitigating all forms of bias,\\nespecially when applied to proprietary or black-box models.\\n5) Summary and Contributions: The existing literature re-\\nveals substantial progress in bias evaluation and mitigation\\nfor LLMs, yet there remain significant challenges, particu-\\nlarly regarding interpretability, dataset alignment, and domain-\\nspecific application. Our research builds on these foundations\\nby applying state-of-the-art bias and fairness frameworks to\\nevaluate multiple open-source LLMs from various organiza-\\ntions, including Meta, Mistral, NousResearch, and Qwen. Our\\nstudy assesses these models across diverse datasets, examining\\nhow their training data and architecture influence bias. This\\nwork contributes to the ongoing efforts to develop fairer\\nand more equitable AI systems, highlighting the nuances and\\nlimitations of current bias evaluation frameworks.\\nIII. M ETHODOLOGY\\nIn this study, we adopted a rigorous methodology to evaluate\\nfairness and bias in large language models (LLMs) developed\\nby multiple leading research organizations. Our evaluation\\nframework consisted of two core components: dataset selection\\nand model assessment using advanced fairness metrics. This\\nsystematic approach was designed to provide a comprehensive\\nunderstanding of how these models perform in terms of equi-\\ntable treatment across different demographic groups, thereby\\ncontributing to the broader discourse on AI fairness and ethical\\nconsiderations in machine learning.\\nWe utilized two benchmark datasets, CivilComments and\\nCOMPAS, which are widely recognized for their applicability\\nin assessing fairness in machine learning models. The Civil-\\nComments dataset contains a collection of online comments\\nthat have been annotated for toxicity and identity labels, thus\\nproviding a rich and varied dataset to evaluate the capabil-\\nity of LLMs to detect and mitigate toxic language without\\nintroducing biases. This dataset includes a broad range of\\nidentity labels, such as gender, race, and religion, allowing\\nfor a nuanced analysis of how various demographic groups\\nare treated by the models. The COMPAS dataset, on the other\\nhand, comprises demographic information and recidivism data\\nused to assess criminal justice risk scores, and has been\\ncentral to numerous discussions regarding fairness in predic-\\ntive algorithms. By incorporating both datasets, we aimed to\\nexamine fairness from multiple perspectives, including racial\\nand gender biases, across distinct domains, ensuring a holistic\\nevaluation of model behavior.\\nThe LLMs evaluated in this study included models from\\nMeta, Mistral, NousResearch, and Qwen. We assessed these\\nmodels using state-of-the-art fairness metrics such as Demo-\\ngraphic Parity, True Positive Rate (TPR) Parity, and False\\nPositive Rate (FPR) Parity. Demographic Parity ensures that\\nthe likelihood of positive predictions is equal across different\\ndemographic groups, indicating that no group is disproportion-\\nately favored or disadvantaged. TPR Parity evaluates whether\\ntrue positive rates are consistent across demographic groups,\\nensuring that the model is equally effective in identifying\\npositive cases irrespective of group membership. FPR Parity\\nmeasures whether false positive rates are balanced across\\ngroups, which is critical for minimizing unjustified negative\\noutcomes for certain populations. Together, these metrics\\nprovide a robust framework for identifying and quantifying\\nbiases in model predictions. To systematically calculate these\\nmetrics and identify disparities, we employed Aequitas, a well-\\nestablished fairness assessment framework. Aequitas offers\\ndetailed breakdowns of fairness metrics and facilitates the\\ncomparison of model behaviors across multiple demographic\\nattributes, making it highly suitable for this type of analysis.\\nOur assessment involved a thorough examination of model\\nperformance across various identity groups, defined by at-\\ntributes such as race, gender, and age, among others. By\\ncomparing the distribution of model outputs and prediction\\npatterns across different groups, we were able to identify\\nspecific areas in which the models demonstrated biases. For\\ninstance, discrepancies in TPR or FPR between groups may in-\\ndicate that the model is systematically more likely to correctly\\nor incorrectly classify instances from certain demographics,\\nwhich has profound implications for fairness in practical, real-\\nworld applications. The inclusion of multiple datasets also\\nenabled us to assess the consistency of these biases across\\ndifferent contexts, providing insights into whether these biases\\nare inherent to the model architecture or are induced by the\\nnature of the training data.\\nOverall, our methodology was not solely focused on iden-\\ntifying the presence of biases but also on uncovering their\\nroot causes. By employing comprehensive fairness metrics and\\nleveraging well-established benchmark datasets, we aimed togenerate a clear and detailed understanding of how LLMs\\nfrom different organizations perform with respect to fairness\\nand equity. These evaluations are crucial for guiding future\\nimprovements in model training and fine-tuning processes,\\nensuring that LLMs can be deployed responsibly and ethically,\\nparticularly in high-stakes applications where fairness is of\\nutmost importance.\\nA. Experimental Setup A\\nFor the evaluation of the CivilComments dataset, we estab-\\nlished a rigorous experimental setup designed to systematically\\nassess the fairness and predictive performance of the selected\\nlarge language models (LLMs). The CivilComments dataset,\\nconsisting of online comments annotated for toxicity and\\nvarious identity labels, was preprocessed and prepared for\\nmodel evaluation to ensure consistency and comparability of\\nresults.\\nWe began by selecting a subset of 5,000 comments from\\nthe test set of the CivilComments dataset to ensure the feasi-\\nbility of the evaluation while maintaining statistical power.\\nThe comments were tokenized using the appropriate tok-\\nenizer for each model, ensuring compatibility and optimal\\ninput representation. To address the challenge of missing or\\ninconsistent tokens, we assigned the padding token to the\\nend-of-sequence token wherever necessary. The preprocessing\\nincluded truncation to a maximum length of 128 tokens to\\nmanage computational resources effectively while preserving\\nkey contextual information.\\nEach model was configured for binary classification to dis-\\ntinguish between toxic and non-toxic comments. The models\\nwere loaded with a consistent configuration, including the\\nsetting of two output labels to represent the binary nature of\\nthe task. During the evaluation, we employed a batch size of\\n256, enabling efficient utilization of available computational\\nresources while ensuring stable convergence of model predic-\\ntions.\\nThe evaluation process utilized a DataLoader with a data\\ncollator that performed dynamic padding, ensuring that input\\nsequences were appropriately aligned for each batch. The\\nmodels were run in inference mode to generate predictions for\\neach comment, and key metrics such as True Positive Rate\\n(TPR), False Positive Rate (FPR), and demographic parity\\nwere calculated to assess model performance across different\\ndemographic groups. This step allowed us to capture both\\naggregate performance metrics and any disparities in outcomes\\nbased on sensitive attributes such as race, gender, or other\\nidentity labels.\\nOverall, the experimental setup for the CivilComments\\ndataset was designed to provide a comprehensive evaluation\\nof fairness and bias across different LLMs, with particular em-\\nphasis on the consistency and comparability of results across\\ndemographic groups. This setup provided critical insights into\\nhow each model handles toxic content and the extent to which\\nbiases may be present in model predictions, highlighting areas\\nfor potential improvement in fairness and equity.\\nB. Experimental Setup B\\nFor the evaluation of the CivilComments dataset, we es-\\ntablished an experimental setup to assess the fairness and\\npredictive performance of large language models (LLMs).\\nThe CivilComments dataset, consisting of online comments\\nannotated for toxicity and identity labels, was preprocessed to\\nensure consistency in model evaluation.\\nWe selected 5,000 comments from the test set for evaluation.\\nComments were tokenized with appropriate tokenizers for each\\nmodel, and truncation was applied to a maximum length of 128\\ntokens to balance computational efficiency and information\\nretention. Padding tokens were added where necessary to\\nmaintain input consistency.\\nEach model was configured for binary classification to\\ndistinguish toxic from non-toxic comments. A batch size of\\n256 was used for efficient resource utilization. The evaluation\\nemployed a DataLoader with dynamic padding to align input\\nsequences and generate predictions, which were then used\\nto calculate metrics such as True Positive Rate (TPR), False\\nPositive Rate (FPR), and demographic parity to assess fairness\\nacross demographic groups.\\nOverall, this setup provided insights into model performance\\non toxic content and highlighted areas for improvement in\\nfairness and equity.\\nFor the COMPAS dataset, we designed an experimental\\nsetup to evaluate LLMs’ predictive performance and fairness\\nin criminal justice risk assessment. The dataset, which contains\\ndemographic information and recidivism data, was used to\\nanalyze the models’ ability to predict recidivism without\\nsignificant demographic bias.\\nWe used the entire training split of COMPAS, including de-\\nmographic features like race, gender, and age, and recidivism\\nlabels. Data preprocessing involved converting categorical\\nattributes into suitable numerical formats. Each model was\\nconfigured for binary classification of recidivism, with a batch\\nsize of 256 for consistency.\\nMetrics such as Demographic Parity, TPR Parity, FPR\\nParity, and False Discovery Rate (FDR) Parity were computed\\nto assess fairness across groups, focusing on race and gender.\\nThese metrics helped identify biases and evaluate fairness in\\nmodel predictions.\\nOverall, this setup provided a thorough evaluation of pre-\\ndictive accuracy and fairness, highlighting areas where biases\\nmay arise and offering insights into improving fairness in AI\\nsystems for high-stakes applications.\\nIV. E MPIRICAL EVALUATION\\nThe empirical evaluation involved running each model on\\nthe prepared datasets and analyzing the resulting predictions.\\nWe compared model performance using fairness metrics, such\\nas demographic parity and TPR/FPR parity, to understand\\ndisparities in model behavior across different demographic\\ngroups. This analysis provided a detailed assessment of both\\nmodel accuracy and fairness, offering valuable insights into\\npotential biases and areas for improvement.A. CivilComments\\nTABLE I\\nTOTAL NUMBER OF OBSERVATIONS 25000\\nModel Demographic Parity TPR FPR FDR A VG\\nLlama 3.1 8B 0.35 0.40 0.45 0.50 0.43\\nLlama 3.1 70B 0.30 0.35 0.40 0.45 0.38\\nLlama 3.2 1B 0.28 0.32 0.38 0.42 0.35\\nLlama 3.2 3B 0.25 0.30 0.34 0.40 0.32\\nMistral 7B 0.55 0.60 0.65 0.70 0.63\\nMixtral 8x7B 0.95 1.00 1.05 1.10 1.03\\nHermes 3 8B 0.15 0.18 0.20 0.22 0.19\\nHermes 3 70B 0.12 0.14 0.16 0.18 0.15\\nQwen 2.5 1.5B 0.45 0.50 0.55 0.60 0.53\\nQwen 2.5 7B 0.50 0.55 0.58 0.62 0.56\\nQwen 2.5 72B 0.48 0.52 0.56 0.60 0.54\\nThe results indicate that Hermes models (Hermes 3 8B and\\nHermes 3 70B) are the least biased, with average bias values\\nof 0.19 and 0.15 respectively, suggesting high fairness across\\nthe metrics. The Llama models also exhibit relatively low bias,\\nespecially the Llama 3.2 versions, with averages ranging from\\n0.32 to 0.35.\\nOn the other hand, Mixtral 8x7B has the highest bias across\\nall metrics, with an average of 1.03, indicating significant\\nfairness concerns. The Qwen models show moderate bias, with\\naverages between 0.53 and 0.56, making them more biased\\ncompared to Hermes and Llama, but still better than Mixtral.\\nOverall, the Hermes models stand out as the most fair, while\\nMixtral requires significant improvements to reduce bias.\\nB. COMPAS\\nTABLE II\\nTOTAL NUMBER OF OBSERVATIONS 4353\\nModel Demographic Parity TPR FPR A VG\\nLlama 3.1 8B 0.40 0.45 0.50 0.45\\nLlama 3.1 70B 0.35 0.40 0.42 0.39\\nLlama 3.2 1B 0.32 0.38 0.40 0.37\\nLlama 3.2 3B 0.30 0.35 0.38 0.34\\nMistral 7B 0.60 0.65 0.68 0.64\\nMixtral 8x7B 1.00 1.05 1.10 1.05\\nHermes 3 8B 0.20 0.22 0.25 0.22\\nHermes 3 70B 0.18 0.20 0.22 0.20\\nQwen 2.5 1.5B 0.50 0.55 0.58 0.54\\nQwen 2.5 7B 0.52 0.56 0.60 0.56\\nQwen 2.5 72B 0.48 0.52 0.55 0.52\\nThe results from this dataset, focusing on racial bias, show\\na similar trend to previous analyses. The Hermes models\\n(Hermes 3 8B and Hermes 3 70B) again display the least bias,\\nwith average values of 0.22 and 0.20, respectively, indicating\\na high level of fairness. The Llama models also demonstrate\\nrelatively low bias, especially Llama 3.2 models, with averages\\nranging from 0.34 to 0.37.\\nMixtral 8x7B exhibits the highest bias in this analysis as\\nwell, with an average of 1.05, suggesting significant fairness\\nissues. The Qwen models show moderate bias, with averages\\nbetween 0.52 and 0.56, making them more biased compared\\nto Hermes and Llama but still less biased than Mixtral.\\nOverall, Hermes models continue to be the most fair, while\\nMixtral models show substantial room for improvement in\\nreducing racial bias.\\nV. C ONCLUSION\\nIn this study, we conducted a comprehensive evaluation of\\nfairness and bias in large language models (LLMs) developed\\nby Meta, Mistral, NousResearch, and Qwen. Using benchmark\\ndatasets such as CivilComments and COMPAS, we applied\\nadvanced fairness metrics, including Demographic Parity, TPR\\nParity, FPR Parity, CUAE, and FDR Parity, to assess the\\nmodels’ behavior across different demographic groups. The\\nresults demonstrated significant disparities in fairness across\\nthe evaluated models, with Hermes models consistently outper-\\nforming others in terms of equitable predictions. Our findings\\nhighlight the importance of employing robust fairness evalu-\\nation frameworks and datasets to identify and mitigate biases\\nin LLMs, particularly in high-stakes applications where biased\\noutcomes can lead to serious social and ethical consequences.\\nFuture work should focus on refining mitigation techniques\\nand developing more transparent, interpretable models that\\ncan better address fairness concerns. Collaboration between\\nresearchers, policymakers, and industry stakeholders is crucial\\nto ensure that AI systems are deployed in ways that promote\\nfairness, equity, and inclusivity.\\nACKNOWLEDGMENT\\nThe authors would like to thank the developers and re-\\nsearch teams at Meta, Mistral, NousResearch, and Qwen for\\nproviding access to their models for evaluation. We also\\nacknowledge the support of the Aequitas framework team for\\ntheir invaluable assistance in applying fairness metrics.\\nREFERENCES\\n1) J. Zhao, M. Fang, S. Pan, W. Yin, and M. Pechenizkiy, ”GPT-\\nBIAS: A Comprehensive Framework for Evaluating Bias in\\nLarge Language Models,” arXiv preprint arXiv:2312.06315v1 ,\\nDec. 2023.\\n2) D. Bouchard, ”An Actionable Framework for Assessing Bias and\\nFairness in Large Language Model Use Cases,” arXiv preprint\\narXiv:2407.10853v2 , Aug. 2024.\\n3) A. F. Oketunji, M. Anas, and D. Saina, ”Large Language Model\\n(LLM) Bias Index—LLMBI,” arXiv preprint arXiv:2312.14769v3 ,\\nDec. 2023.\\n4) W. Jeung, D. Jeon, A. Yousefpour, and J. Choi, ”Large Lan-\\nguage Models Still Exhibit Bias in Long Text,” arXiv preprint\\narXiv:2410.17519v2 , Oct. 2024.\\n',\n",
       " 'number': 5,\n",
       " 'subject': 'Artifical Intelligence',\n",
       " 'tone': 'Simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"response\": \"correct answer\"}}',\n",
       " 'quiz': '{\"1\": {\"mcq\": \"Which large language models were evaluated in the study for fairness and bias?\", \"options\": {\"a\": \"Meta, Mistral, NousResearch, and Qwen\", \"b\": \"Google, IBM, Amazon, and Microsoft\", \"c\": \"OpenAI, DeepMind, Facebook AI, and Apple AI\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"2\": {\"mcq\": \"Which benchmark datasets were used in the study?\", \"options\": {\"a\": \"IMDB and Yelp\", \"b\": \"Twitter and Facebook\", \"c\": \"CivilComments and COMPAS\", \"d\": \"None of the above\"}, \"response\": \"c\"}, \"3\": {\"mcq\": \"Which model consistently demonstrated higher fairness compared to others?\", \"options\": {\"a\": \"Hermes\", \"b\": \"Llama\", \"c\": \"Mistral\", \"d\": \"Qwen\"}, \"response\": \"a\"}, \"4\": {\"mcq\": \"What is the Demographic Parity metric used for?\", \"options\": {\"a\": \"To ensure that the likelihood of positive predictions is equal across different demographic groups\", \"b\": \"To evaluate whether true positive rates are consistent across demographic groups\", \"c\": \"To measure whether false positive rates are balanced across groups\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"5\": {\"mcq\": \"What does a high value in the fairness metrics indicate?\", \"options\": {\"a\": \"High bias\", \"b\": \"Low bias\", \"c\": \"No bias\", \"d\": \"None of the above\"}, \"response\": \"a\"}}',\n",
       " 'review': 'Complexity Analysis: The quiz is moderately complex, requiring specific knowledge of AI language models, fairness metrics, and demographic parity. \\n\\nUpdated Quiz_MCQs:\\n{\"1\": {\"mcq\": \"Which AI language models were analyzed for fairness and bias in the study?\", \"options\": {\"a\": \"Meta, Mistral, NousResearch, and Qwen\", \"b\": \"Google, IBM, Amazon, and Microsoft\", \"c\": \"OpenAI, DeepMind, Facebook AI, and Apple AI\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"2\": {\"mcq\": \"Which datasets were utilized in the study?\", \"options\": {\"a\": \"IMDB and Yelp\", \"b\": \"Twitter and Facebook\", \"c\": \"CivilComments and COMPAS\", \"d\": \"None of the above\"}, \"response\": \"c\"}, \"3\": {\"mcq\": \"Which model showed superior fairness compared to others?\", \"options\": {\"a\": \"Hermes\", \"b\": \"Llama\", \"c\": \"Mistral\", \"d\": \"Qwen\"}, \"response\": \"a\"}, \"4\": {\"mcq\": \"What is the purpose of the Demographic Parity metric?\", \"options\": {\"a\": \"To ensure that the likelihood of positive predictions is equal across different demographic groups\", \"b\": \"To evaluate whether true positive rates are consistent across demographic groups\", \"c\": \"To measure whether false positive rates are balanced across groups\", \"d\": \"None of the above\"}, \"response\": \"a\"}, \"5\": {\"mcq\": \"What does a high value in the fairness metrics suggest?\", \"options\": {\"a\": \"High bias\", \"b\": \"Low bias\", \"c\": \"No bias\", \"d\": \"None of the above\"}, \"response\": \"a\"}}'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = response.get('quiz').replace('###RESPONSE_JSON\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=json.loads(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('1', {'mcq': 'Which large language models were evaluated in the study for fairness and bias?', 'options': {'a': 'Meta, Mistral, NousResearch, and Qwen', 'b': 'Google, IBM, Amazon, and Microsoft', 'c': 'OpenAI, DeepMind, Facebook AI, and Apple AI', 'd': 'None of the above'}, 'response': 'a'}), ('2', {'mcq': 'Which benchmark datasets were used in the study?', 'options': {'a': 'IMDB and Yelp', 'b': 'Twitter and Facebook', 'c': 'CivilComments and COMPAS', 'd': 'None of the above'}, 'response': 'c'}), ('3', {'mcq': 'Which model consistently demonstrated higher fairness compared to others?', 'options': {'a': 'Hermes', 'b': 'Llama', 'c': 'Mistral', 'd': 'Qwen'}, 'response': 'a'}), ('4', {'mcq': 'What is the Demographic Parity metric used for?', 'options': {'a': 'To ensure that the likelihood of positive predictions is equal across different demographic groups', 'b': 'To evaluate whether true positive rates are consistent across demographic groups', 'c': 'To measure whether false positive rates are balanced across groups', 'd': 'None of the above'}, 'response': 'a'}), ('5', {'mcq': 'What does a high value in the fairness metrics indicate?', 'options': {'a': 'High bias', 'b': 'Low bias', 'c': 'No bias', 'd': 'None of the above'}, 'response': 'a'})])\n"
     ]
    }
   ],
   "source": [
    "print(quiz.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "    )\n",
    "    correct = value[\"response\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'Which large language models were evaluated in the study for fairness and bias?',\n",
       "  'Choices': 'a: Meta, Mistral, NousResearch, and Qwen | b: Google, IBM, Amazon, and Microsoft | c: OpenAI, DeepMind, Facebook AI, and Apple AI | d: None of the above',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'Which benchmark datasets were used in the study?',\n",
       "  'Choices': 'a: IMDB and Yelp | b: Twitter and Facebook | c: CivilComments and COMPAS | d: None of the above',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'Which model consistently demonstrated higher fairness compared to others?',\n",
       "  'Choices': 'a: Hermes | b: Llama | c: Mistral | d: Qwen',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'What is the Demographic Parity metric used for?',\n",
       "  'Choices': 'a: To ensure that the likelihood of positive predictions is equal across different demographic groups | b: To evaluate whether true positive rates are consistent across demographic groups | c: To measure whether false positive rates are balanced across groups | d: None of the above',\n",
       "  'Correct': 'a'},\n",
       " {'MCQ': 'What does a high value in the fairness metrics indicate?',\n",
       "  'Choices': 'a: High bias | b: Low bias | c: No bias | d: None of the above',\n",
       "  'Correct': 'a'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mcq_question.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
